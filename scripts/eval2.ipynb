{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 110 outputs\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# gpt-4o-2024-11-20, claude-3-5-sonnet-20241022, gemini-1.5-pro, gpt-4o-mini, deepseek-reasoner...\n",
    "model = \"mistralai/mistral-small-3.1-24b-instruct\"\n",
    "\n",
    "dataset = \"nfqa_rag\"\n",
    "\n",
    "split_list = [\"text\"]\n",
    "\n",
    "full_outputs = []\n",
    "\n",
    "result_path = f\"outputs/{dataset}/{model}/{dataset}/zero_shot/cot/{dataset}_output.jsonl\"\n",
    "\n",
    "with open(result_path, \"r\") as f:\n",
    "    outputs = [json.loads(line) for line in f]\n",
    "\n",
    "source_path = f\"data/{dataset}/input/{dataset}_input.jsonl\"\n",
    "with open(source_path, \"r\") as f:\n",
    "    sources = [json.loads(line) for line in f]\n",
    "assert len(sources) == len(outputs)\n",
    "for i, source in enumerate(sources):\n",
    "    assert source['id'] == outputs[i]['id']\n",
    "    assert source['question'] == outputs[i]['question']\n",
    "\n",
    "print(f\"Loaded {len(outputs)} outputs\")\n",
    "full_outputs.extend(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_string(s):\n",
    "    parts = re.split(r'(?i)final answer', s)\n",
    "    return parts\n",
    "\n",
    "if \"qvq\" in model.lower():\n",
    "    print(model)\n",
    "    new_data = []\n",
    "    for index, line in enumerate(tqdm(full_outputs)):\n",
    "        prediction_rationale = line[\"messages\"][-1][\"content\"]\n",
    "\n",
    "        if re.search(r'(?i)final answer', prediction_rationale):\n",
    "            flag = True\n",
    "        else:\n",
    "            flag = False\n",
    "\n",
    "        prediction = split_string(prediction_rationale)[-1].strip()\n",
    "\n",
    "        if line['id'].lower().startswith(\"text\"):\n",
    "            u_pattern = r\"[A-J]\"\n",
    "            l_pattern = r\"[a-j]\"\n",
    "        else:\n",
    "            u_pattern = r\"[A-E]\"\n",
    "            l_pattern = r\"[a-e]\"\n",
    "\n",
    "        letter_match = re.findall(u_pattern, prediction)\n",
    "        if letter_match:\n",
    "            if flag:\n",
    "                prediction = letter_match[0]\n",
    "            else:\n",
    "                prediction = letter_match[-1]\n",
    "        else:\n",
    "            letter_match = re.findall(l_pattern, prediction)\n",
    "            if letter_match:\n",
    "                if flag:\n",
    "                    prediction = letter_match[0].upper()\n",
    "                else:\n",
    "                    prediction = letter_match[-1].upper()\n",
    "\n",
    "        label = line[\"label\"][0]\n",
    "        line[\"prediction\"] = prediction\n",
    "        line[\"correct\"] = prediction == label\n",
    "        new_data.append(line)\n",
    "    full_outputs = new_data\n",
    "elif model == \"deepseek-reasoner\":\n",
    "    print(model)\n",
    "    new_data = []\n",
    "    for index, line in enumerate(tqdm(full_outputs)):\n",
    "        assert \"Put your final\" in line['messages'][-2][\"content\"]\n",
    "\n",
    "        prediction = line['response']\n",
    "\n",
    "        if line['id'].lower().startswith(\"text\"):\n",
    "            pattern = r\"\\\\boxed{([A-J])}\"\n",
    "        else:\n",
    "            pattern = r\"\\\\boxed{([A-E])}\"\n",
    "\n",
    "        letter_match = re.findall(pattern, prediction)\n",
    "        prediction = letter_match[0] if letter_match else prediction\n",
    "        label = line['label'][0]\n",
    "        line[\"prediction\"] = prediction\n",
    "        line[\"correct\"] = prediction == label\n",
    "        new_data.append(line)\n",
    "    full_outputs = new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: mistralai/mistral-small-3.1-24b-instruct\n",
      "------------------- Main Results -------------------\n",
      "Model: mistralai/mistral-small-3.1-24b-instruct\n",
      "Accuracy: 0.7182 (79/110)\n"
     ]
    }
   ],
   "source": [
    "# ... existing code ...\n",
    "\n",
    "# Results stats\n",
    "print(f\"Model: {model}\")\n",
    "\n",
    "\n",
    "\n",
    "# Results stats\n",
    "print(\"------------------- Main Results -------------------\")\n",
    "print(f\"Model: {model}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_count = sum(1 for item in full_outputs if item.get(\"correct\", False))\n",
    "total_count = len(full_outputs)\n",
    "accuracy = correct_count / total_count if total_count > 0 else 0\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f} ({correct_count}/{total_count})\")\n",
    "\n",
    "# Optional: Calculate accuracy by question type if applicable\n",
    "if any(item['id'].lower().startswith(\"text\") for item in full_outputs):\n",
    "    text_items = [item for item in full_outputs if item['id'].lower().startswith(\"text\")]\n",
    "    text_correct = sum(1 for item in text_items if item.get(\"correct\", False))\n",
    "    \n",
    "    non_text_items = [item for item in full_outputs if not item['id'].lower().startswith(\"text\")]\n",
    "    non_text_correct = sum(1 for item in non_text_items if item.get(\"correct\", False))\n",
    "    \n",
    "    print(f\"Text questions accuracy: {text_correct/len(text_items):.4f} ({text_correct}/{len(text_items)})\")\n",
    "    print(f\"Other questions accuracy: {non_text_correct/len(non_text_items):.4f} ({non_text_correct}/{len(non_text_items)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- NFQA Results -------------------\n",
      "Model: openai/gpt-4o-mini\n",
      "Accuracy: 0.7523 (82/109)\n",
      "Model: deepseek/deepseek-r1-distill-qwen-32b\n",
      "Accuracy: 0.5872 (64/109)\n",
      "Model: mistralai/mistral-small-3.1-24b-instruct\n",
      "Accuracy: 0.6606 (72/109)\n",
      "Model: google/gemini-2.0-flash-001\n",
      "Accuracy: 0.7064 (77/109)\n",
      "Model: anthropic/claude-3.5-haiku\n",
      "Accuracy: 0.6606 (72/109)\n",
      "Model: meta-llama/llama-3.2-3b-instruct\n",
      "Accuracy: 0.6239 (68/109)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = \"nfqa\"\n",
    "\n",
    "print(\"------------------- NFQA Results -------------------\")\n",
    "\n",
    "model_list = [\"openai/gpt-4o-mini\",\n",
    "              \"deepseek/deepseek-r1-distill-qwen-32b\",\n",
    "              \"mistralai/mistral-small-3.1-24b-instruct\", \n",
    "              \"google/gemini-2.0-flash-001\", \n",
    "              \"anthropic/claude-3.5-haiku\",\n",
    "              \"meta-llama/llama-3.2-3b-instruct\"]\n",
    "\n",
    "for model in model_list:\n",
    "    result_path = f\"outputs/{dataset}/{model}/{dataset}/zero_shot/cot/{dataset}_output.jsonl\"\n",
    "    with open(result_path, \"r\") as f:\n",
    "        outputs = [json.loads(line) for line in f]\n",
    "    print(f\"Model: {model}\")\n",
    "    if outputs == []:\n",
    "        print(\"N/A\")\n",
    "    else:\n",
    "        print(f\"Accuracy: {sum(1 for item in outputs if item.get('correct', False)) / len(outputs):.4f} ({sum(1 for item in outputs if item.get('correct', False))}/{len(outputs)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- NFQA RAG Results -------------------\n",
      "Model: openai/gpt-4o-mini\n",
      "Accuracy: 0.8818 (97/110)\n",
      "Model: deepseek/deepseek-r1-distill-qwen-32b\n",
      "Accuracy: 0.6818 (75/110)\n",
      "Model: mistralai/mistral-small-3.1-24b-instruct\n",
      "Accuracy: 0.7182 (79/110)\n",
      "Model: google/gemini-2.0-flash-001\n",
      "Accuracy: 0.8364 (92/110)\n",
      "Model: anthropic/claude-3.5-haiku\n",
      "Accuracy: 0.7909 (87/110)\n",
      "Model: meta-llama/llama-3.2-3b-instruct\n",
      "Accuracy: 0.7909 (87/110)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = \"nfqa_rag\"\n",
    "\n",
    "print(\"------------------- NFQA RAG Results -------------------\")\n",
    "\n",
    "model_list = [\"openai/gpt-4o-mini\",\n",
    "              \"deepseek/deepseek-r1-distill-qwen-32b\",\n",
    "              \"mistralai/mistral-small-3.1-24b-instruct\", \n",
    "              \"google/gemini-2.0-flash-001\", \n",
    "              \"anthropic/claude-3.5-haiku\",\n",
    "              \"meta-llama/llama-3.2-3b-instruct\"]\n",
    "\n",
    "for model in model_list:\n",
    "    result_path = f\"outputs/{dataset}/{model}/{dataset}/zero_shot/cot/{dataset}_output.jsonl\"\n",
    "    with open(result_path, \"r\") as f:\n",
    "        outputs = [json.loads(line) for line in f]\n",
    "    print(f\"Model: {model}\")\n",
    "    if outputs == []:\n",
    "        print(\"N/A\")\n",
    "    else:\n",
    "        print(f\"Accuracy: {sum(1 for item in outputs if item.get('correct', False)) / len(outputs):.4f} ({sum(1 for item in outputs if item.get('correct', False))}/{len(outputs)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

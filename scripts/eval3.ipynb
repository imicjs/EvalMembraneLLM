{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/masa/miniforge3/envs/torch/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(predictions, labels):\n",
    "    # Initialize ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    # Calculate ROUGE scores\n",
    "    rouge_scores = {\n",
    "        'rouge1': [],\n",
    "        'rouge2': [],\n",
    "        'rougeL': []\n",
    "    }\n",
    "    \n",
    "    for pred, label in zip(predictions, labels):\n",
    "        scores = scorer.score(label, pred)\n",
    "        rouge_scores['rouge1'].append(scores['rouge1'].fmeasure)\n",
    "        rouge_scores['rouge2'].append(scores['rouge2'].fmeasure)\n",
    "        rouge_scores['rougeL'].append(scores['rougeL'].fmeasure)\n",
    "    \n",
    "    # Calculate average ROUGE scores\n",
    "    avg_rouge_scores = {\n",
    "        'rouge1': np.mean(rouge_scores['rouge1']),\n",
    "        'rouge2': np.mean(rouge_scores['rouge2']),\n",
    "        'rougeL': np.mean(rouge_scores['rougeL'])\n",
    "    }\n",
    "\n",
    "     # Calculate BERTScore\n",
    "    P, R, F1 = score(predictions, labels, lang='en', verbose=False)\n",
    "    bert_f1 = torch.mean(F1).item()\n",
    "    \n",
    "    # Calculate exact match F1\n",
    "    exact_matches = [1 if pred == label else 0 for pred, label in zip(predictions, labels)]\n",
    "    exact_f1 = np.mean(exact_matches)\n",
    "    \n",
    "    return {\n",
    "        'exact_f1': exact_f1,\n",
    "        'rouge1': avg_rouge_scores['rouge1'],\n",
    "        'rouge2': avg_rouge_scores['rouge2'], \n",
    "        'rougeL': avg_rouge_scores['rougeL'],\n",
    "        'bert_score': bert_f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Metrics:\n",
      "Exact Match F1: 0.0000\n",
      "ROUGE-1: 0.1242\n",
      "ROUGE-2: 0.0475\n",
      "ROUGE-L: 0.1002\n",
      "BERTScore: 0.8224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    }
   ],
   "source": [
    "input_data = load_jsonl('data/nfqa_open/input/nfqa_open_input.jsonl')\n",
    "output_data = load_jsonl('outputs/nfqa_open/anthropic/nfqa_open/zero_shot/cot/nfqa_open_output.jsonl')\n",
    "\n",
    "# Extract predictions and labels\n",
    "predictions = [item['prediction'] for item in output_data]\n",
    "labels = [item['label'][0] if isinstance(item['label'], list) else item['label'] for item in input_data]\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = calculate_metrics(predictions, labels)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(f\"Exact Match F1: {metrics['exact_f1']:.4f}\")\n",
    "print(f\"ROUGE-1: {metrics['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2: {metrics['rouge2']:.4f}\") \n",
    "print(f\"ROUGE-L: {metrics['rougeL']:.4f}\")\n",
    "print(f\"BERTScore: {metrics['bert_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 17%|█▋        | 1/6 [00:35<02:56, 35.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "mistralai Exact Match F1: 0.0000\n",
      "mistralai ROUGE-1: 0.1597\n",
      "mistralai ROUGE-2: 0.0689\n",
      "mistralai ROUGE-L: 0.1294\n",
      "mistralai BERTScore: 0.8417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 33%|███▎      | 2/6 [00:51<01:35, 23.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "google Exact Match F1: 0.0000\n",
      "google ROUGE-1: 0.2531\n",
      "google ROUGE-2: 0.1145\n",
      "google ROUGE-L: 0.2100\n",
      "google BERTScore: 0.8605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      " 50%|█████     | 3/6 [01:24<01:24, 28.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "deepseek Exact Match F1: 0.0000\n",
      "deepseek ROUGE-1: 0.1242\n",
      "deepseek ROUGE-2: 0.0475\n",
      "deepseek ROUGE-L: 0.1002\n",
      "deepseek BERTScore: 0.8224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 67%|██████▋   | 4/6 [01:49<00:53, 26.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "anthropic Exact Match F1: 0.0000\n",
      "anthropic ROUGE-1: 0.1662\n",
      "anthropic ROUGE-2: 0.0668\n",
      "anthropic ROUGE-L: 0.1297\n",
      "anthropic BERTScore: 0.8443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 83%|████████▎ | 5/6 [02:06<00:23, 23.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "meta-llama Exact Match F1: 0.0000\n",
      "meta-llama ROUGE-1: 0.2811\n",
      "meta-llama ROUGE-2: 0.1312\n",
      "meta-llama ROUGE-L: 0.2357\n",
      "meta-llama BERTScore: 0.8658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 6/6 [02:41<00:00, 26.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "openai Exact Match F1: 0.0000\n",
      "openai ROUGE-1: 0.1928\n",
      "openai ROUGE-2: 0.0790\n",
      "openai ROUGE-L: 0.1550\n",
      "openai BERTScore: 0.8487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "input_data = load_jsonl('data/nfqa_open/input/nfqa_open_input.jsonl')\n",
    "\n",
    "\n",
    "for model in tqdm(os.listdir('outputs/nfqa_open')):\n",
    "    sub_dir = os.listdir('outputs/nfqa_open/'+ model)\n",
    "    output_data = load_jsonl('outputs/nfqa_open/'+ model +'/'+ sub_dir[0] +'/nfqa_open/zero_shot/cot/nfqa_open_output.jsonl')\n",
    "    \n",
    "    predictions = [item['prediction'] for item in output_data]\n",
    "    labels = [item['label'][0] if isinstance(item['label'], list) else item['label'] for item in input_data]\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = calculate_metrics(predictions, labels)\n",
    "\n",
    "    # Print results\n",
    "    # print(\"\\nEvaluation Metrics:\")\n",
    "    print('\\n\\n')\n",
    "    print(f\"{model} Exact Match F1: {metrics['exact_f1']:.4f}\")\n",
    "    print(f\"{model} ROUGE-1: {metrics['rouge1']:.4f}\")\n",
    "    print(f\"{model} ROUGE-2: {metrics['rouge2']:.4f}\") \n",
    "    print(f\"{model} ROUGE-L: {metrics['rougeL']:.4f}\")\n",
    "    print(f\"{model} BERTScore: {metrics['bert_score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def compute_f1(pred_tokens, gt_tokens):\n",
    "    common_tokens = set(pred_tokens) & set(gt_tokens)\n",
    "    num_common = len(common_tokens)\n",
    "\n",
    "    if num_common == 0:\n",
    "        return 0.0\n",
    "\n",
    "    precision = num_common / len(pred_tokens)\n",
    "    recall = num_common / len(gt_tokens)\n",
    "\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "# adopted from opencompass\n",
    "def general_postprocess(text: str) -> str:\n",
    "    # Cut off the first newline, period, or comma\n",
    "    truncated_text = re.split(r\"[\\n.,]\", text, 1)[0]\n",
    "\n",
    "    # Remove punctuation\n",
    "    no_punctuation = re.sub(r\"[^\\w\\s]\", \"\", truncated_text)\n",
    "\n",
    "    # Remove article\n",
    "    no_articles = re.sub(r\"\\b(a|an|the)\\b\", \"\", no_punctuation, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove duplicated blank spaces\n",
    "    cleaned_text = re.sub(r\"\\s+\", \" \", no_articles).strip()\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def tokenize_and_lemmatize(text: str) -> list:\n",
    "    text = text.lower()\n",
    "    text = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in text]\n",
    "    return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 89.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "mistralai Adjust F1: 0.1408\n",
      "\n",
      "\n",
      "\n",
      "google Adjust F1: 0.2237\n",
      "\n",
      "\n",
      "\n",
      "deepseek Adjust F1: 0.1110\n",
      "\n",
      "\n",
      "\n",
      "anthropic Adjust F1: 0.1452\n",
      "\n",
      "\n",
      "\n",
      "meta-llama Adjust F1: 0.2539\n",
      "\n",
      "\n",
      "\n",
      "openai Adjust F1: 0.1645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from token_f1 import calculate_token_f1\n",
    "\n",
    "\n",
    "input_data = load_jsonl('data/nfqa_open/input/nfqa_open_input.jsonl')\n",
    "\n",
    "\n",
    "for model in tqdm(os.listdir('outputs/nfqa_open')):\n",
    "    sub_dir = os.listdir('outputs/nfqa_open/'+ model)\n",
    "    output_data = load_jsonl('outputs/nfqa_open/'+ model +'/'+ sub_dir[0] +'/nfqa_open/zero_shot/cot/nfqa_open_output.jsonl')\n",
    "    \n",
    "    predictions = [item['prediction'] for item in output_data]\n",
    "    labels = [item['label'][0] if isinstance(item['label'], list) else item['label'] for item in input_data]\n",
    "\n",
    "    f1_score = calculate_token_f1(predictions, labels)\n",
    "\n",
    "    # Print results\n",
    "    # print(\"\\nEvaluation Metrics:\")\n",
    "    print('\\n\\n')\n",
    "    print(f\"{model} Adjust F1: {f1_score:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
